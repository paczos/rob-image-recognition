% this small NN for tiny data should be 100% correct

% now you can (probably) play with ann_training

% use PCA to reduce dimensionality  
% other method: use PCA and use all dimensions -> decorrelate data

% results must be better than the reference, 
% the logic of classification function could be different, now a simple max is used
% 
% passing: basic backpropagation for a network, calc derivative, 
% 
% fastest path may not be the best in this case, more than one element
% this ds has never been used before, free to suggest new methods
% 
% when to stop trainig? maybe we should wait longer?
%
% seeded random -> in project files there should be serialized state of generator -> repeatability
% 
% 
% 
%


% best so far: 0.114200



[tvec tlab tstv tstl] = readSets();
limSize = rows(tvec);
%limSize = 1000;
limIdx = randperm(limSize);

tlab += 1;
tstl += 1;

tlab = tlab(limIdx, :);
tvec = tvec(limIdx, :);

tvec = normalize(tvec);
tstv = normalize(tstv);

noHiddenNeurons = 400;
noEpochs = 200;
learningRate = 0.1;
learningDropFrac = 0.1;
minDropRate = 0.0005;
regularization = 0.001;

rand()
rndstate = rand("state");
% save rndstate.txt rndstate
load rndstate.txt
rand("state", rndstate);

[hlnn olnn] = crann(columns(tvec), noHiddenNeurons, 10);
trainError = zeros(1, noEpochs);
testError = zeros(1, noEpochs);
trReport = [];

prevError = 1;
for epoch=1:noEpochs
	tic();
	[hlnn olnn terr] = backprop(tvec, tlab, hlnn, olnn, learningRate, regularization);
	clsRes = anncls(tvec, hlnn, olnn);
	cfmx = confMx(tlab, clsRes);   % it is worth looking into the cfmx in order to know which clothes are incorrectly classified
	errcf = compErrors(cfmx);
	trainError(epoch) = errcf(2);

	clsRes = anncls(tstv, hlnn, olnn);
	cfmx = confMx(tstl, clsRes);
	errcf2 = compErrors(cfmx);
	testError(epoch) = errcf2(2);
	epochTime = toc();
	disp([epoch epochTime trainError(epoch) testError(epoch)])
	trReport = [trReport; epoch epochTime trainError(epoch) testError(epoch)];

	if (trainError(epoch) >= prevError)
        if(minDropRate < learningRate)
            learningRate *= (1-learningDropFrac);
            disp("learningRate dropped to")
            learningRate
        end
	end

    prevError = trainError(epoch);
	fflush(stdout);
end


% to beat:
% train: 90.38% 9.62% 0.00%
% test: 87.42% 12.58% 0.00%

% vanilla implementation + params tweaking:
% 0.058100     0.111900

%noHiddenNeurons = 200;
%noEpochs = 50;
%learningRate = 0.01;
%ans =  0.17400
%    1.00000   79.63110    0.23483    0.00000
%    2.00000   79.52931    0.18032    0.00000
%   3.00000   79.36038    0.16568    0.00000



%noHiddenNeurons = 400;
%noEpochs = 60;
%learningRate = 0.015;

%cnt =  60000
%nel =  60000
%cnt =  10000
%nel =  10000
%limSize =  60000
%ans =  0.99220
%     1.00000   158.12970     0.19852     0.20910
%     2.00000   158.73796     0.16570     0.17950
%     3.00000   158.90055     0.15362     0.16870
%     4.00000   158.75812     0.14622     0.16340
%     5.00000   159.11237     0.14167     0.15790
%     6.00000   158.16983     0.13813     0.15320
%     7.00000   158.66217     0.13397     0.15030
%     8.00000   158.99951     0.13125     0.14790
%     9.00000   158.16743     0.12847     0.14690
%    10.00000   158.67155     0.12585     0.14500
%    11.00000   158.09115     0.12380     0.14230
%    12.00000   158.43752     0.12172     0.14170
%    13.00000   157.91327     0.12007     0.14010
%    14.00000   157.85690     0.11817     0.13840
%    15.00000   158.38921     0.11640     0.13760
%    16.00000   158.06481     0.11453     0.13570
%    17.00000   158.19411     0.11290     0.13410
%    18.00000   158.23308     0.11135     0.13340
%    19.00000   156.77038     0.10965     0.13270
%    20.00000   159.27643     0.10803     0.13180
%    21.00000   156.86002     0.10663     0.13100
%    22.00000   156.72536     0.10567     0.12990
%    23.00000   157.16665     0.10453     0.12940
%    24.00000   157.15799     0.10350     0.12820
%    25.00000   156.60208     0.10230     0.12750
%    26.00000   156.70280     0.10137     0.12680
%    27.00000   156.93318     0.10058     0.12540
%    28.000000   157.205459     0.099400     0.124800
%    29.000000   156.754662     0.098233     0.124600
%    30.000000   156.397384     0.097417     0.123100
%    31.000000   156.759514     0.096267     0.123100
%    32.000000   156.293580     0.095283     0.122700
%    33.000000   156.751140     0.094433     0.122500
%    34.000000   157.106400     0.093500     0.121900
%    35.000000   157.000432     0.092483     0.121400
%    36.000000   156.894188     0.091600     0.121200
%    37.000000   156.674338     0.090800     0.121000
%    38.000000   156.887200     0.089917     0.120500
%    39.000000   157.058863     0.089183     0.119500
%    40.000000   156.684943     0.088167     0.119400
%    41.000000   157.944792     0.087383     0.119200
%    42.000000   157.033987     0.086450     0.118700
%    43.000000   157.632600     0.085517     0.118700
%    44.000000   157.369003     0.084867     0.118300
%    45.000000   156.518994     0.083850     0.118900
%    46.000000   156.914155     0.082883     0.119000
%    47.000000   157.467678     0.081900     0.118300
%    48.000000   156.623075     0.081133     0.117900
%    49.000000   157.144239     0.080233     0.117200
%    50.000000   156.344235     0.079633     0.116700
%    51.000000   157.155496     0.078917     0.116300
%    52.000000   156.682843     0.078283     0.116500
%    53.000000   156.637566     0.077717     0.116300
%    54.000000   156.764728     0.077033     0.115700
%    55.000000   156.958661     0.076400     0.115200
%    56.000000   157.207272     0.075783     0.114500
%    57.000000   156.944841     0.075167     0.114400
%    58.000000   156.211678     0.074350     0.114400
%    59.000000   156.574942     0.073817     0.114100
%    60.000000   156.913653     0.073183     0.114200

% used same params but smaller train set:
%nel =  60000
%cnt =  10000
%nel =  10000
%ans =  0.49906
%   1.00000   3.12805   0.89600   0.90000
%   2.00000   3.05713   0.89600   0.90000
%   3.00000   3.07440   0.88300   0.89150
%   4.00000   3.09342   0.69600   0.72300
%   5.00000   3.07737   0.62800   0.64830
%   6.00000   3.08353   0.52700   0.54490
%   7.00000   3.11141   0.43300   0.45960
%   8.00000   3.06046   0.39800   0.42260
%   9.00000   3.11653   0.38900   0.40850
%   10.00000    3.10169    0.37200    0.39900
%   11.00000    3.07615    0.35900    0.38940
%   12.00000    3.19004    0.33400    0.37820
%   13.00000    3.14540    0.31100    0.35950
%   14.00000    3.11769    0.30400    0.33880
%   15.00000    3.04258    0.28200    0.32170
%   16.00000    3.05758    0.27100    0.30860
%   17.00000    3.00996    0.26300    0.30090
%   18.00000    3.06576    0.25700    0.29620
%   19.00000    3.02580    0.24800    0.28940
%   20.00000    3.02396    0.24600    0.28440
%   21.00000    3.04087    0.24500    0.27990
%   22.00000    3.09016    0.24100    0.27670
%   23.00000    3.02172    0.23600    0.27310
%   24.00000    3.05828    0.23100    0.27240
%   25.00000    3.01759    0.22800    0.26990
%   26.00000    3.06436    0.22400    0.26780
%   27.00000    3.05142    0.21900    0.26540
%   28.00000    3.08287    0.21500    0.26390
%   29.00000    3.09490    0.21400    0.26190
%   30.00000    3.02824    0.20900    0.25940
%   31.00000    3.01382    0.20700    0.25780
%   32.00000    3.06430    0.20200    0.25570
%   33.00000    3.04883    0.19800    0.25280
%   34.00000    3.10517    0.19500    0.25040
%   35.00000    3.05765    0.19200    0.24760
%   36.00000    3.15294    0.18700    0.24580
%   37.00000    3.16059    0.18300    0.24340
%   38.00000    3.22274    0.18100    0.24170
%   39.00000    3.08262    0.17800    0.23900
%   40.00000    3.09476    0.17600    0.23690
%   41.00000    3.05108    0.17100    0.23540
%   42.00000    3.11462    0.16700    0.23460
%   43.00000    3.05425    0.16400    0.23350
%   44.00000    3.04017    0.16200    0.23360
%   45.00000    3.10172    0.15800    0.23230
%   46.00000    3.09367    0.15200    0.23100
%   47.00000    3.04131    0.15000    0.23050
%   48.00000    3.04957    0.14700    0.23010
%   49.00000    3.01380    0.14200    0.22790
%   50.00000    3.01575    0.13900    0.22670
%   51.00000    3.04830    0.13700    0.22570
%   52.00000    3.02527    0.13600    0.22450
%   53.00000    3.04452    0.13600    0.22300
%   54.00000    3.03607    0.13200    0.22230
%   55.00000    3.11844    0.13100    0.22150
%   56.00000    3.15210    0.12600    0.22140
%   57.00000    3.05406    0.12400    0.22090
%   58.00000    3.09013    0.12300    0.22080
%   59.00000    3.03454    0.12300    0.22040
%   60.00000    3.00699    0.12200    0.21970


%noHiddenNeurons = 400;
%noEpochs = 70;
%learningRate = 0.025;
%
%     1.00000   157.62863     0.18157     0.19360
%     2.00000   158.15464     0.15988     0.17290
%     3.00000   158.34585     0.14875     0.16200
%     4.00000   158.50825     0.14017     0.15430
%     5.00000   157.93592     0.13372     0.15060
%     6.00000   157.73897     0.12867     0.14450
%     7.00000   158.41189     0.12420     0.14180
%     8.00000   158.54762     0.12120     0.13850
%     9.00000   158.59928     0.11798     0.13540
%    10.00000   158.15023     0.11540     0.13340
%    11.00000   157.84903     0.11307     0.13190
%    12.00000   158.10328     0.11097     0.13150
%    13.00000   157.89401     0.10855     0.13010
%    14.00000   158.34232     0.10633     0.12860
%    15.00000   158.16199     0.10430     0.12740
%    16.00000   157.69957     0.10222     0.12630
%    17.00000   158.17655     0.10067     0.12420
%    18.000000   157.957865     0.099217     0.123700
%    19.000000   159.122827     0.097667     0.122500
%    20.000000   158.687877     0.096283     0.122700
%    21.000000   158.625791     0.094783     0.122400
%    22.000000   159.110525     0.093050     0.121400
%    23.000000   158.302390     0.091967     0.120500
%    24.000000   159.421185     0.090450     0.119900
%    25.000000   157.585951     0.089117     0.119800
%    26.000000   158.468990     0.087900     0.119700
%    27.000000   157.785390     0.086967     0.118800
%    28.000000   158.821688     0.085783     0.117700
%    29.000000   160.719119     0.084817     0.116800
%    30.000000   160.664733     0.083900     0.116800
%    31.000000   159.841529     0.082817     0.116900
%    32.000000   160.815021     0.081783     0.116000
%    33.000000   161.163265     0.080967     0.116000
%    34.000000   160.955891     0.080000     0.116100
%    35.000000   159.626924     0.079183     0.115400
%    36.000000   159.246136     0.078483     0.115100
%    37.000000   160.250138     0.077767     0.115600
%    38.000000   161.188235     0.076967     0.116400
%    39.000000   159.220070     0.076600     0.116000
%    40.000000   160.098277     0.075817     0.116000
%    41.000000   162.508317     0.074983     0.115400
%    42.000000   160.103202     0.074233     0.115300
%    43.000000   159.342916     0.073300     0.115400
%    44.000000   159.737320     0.072683     0.115500
%    45.000000   161.014097     0.071817     0.115400
%    46.000000   159.447582     0.071267     0.114900
%    47.000000   160.582516     0.070517     0.114200
%    48.000000   159.197235     0.069400     0.113800
%    49.000000   159.195799     0.068767     0.113900
%    50.000000   161.349482     0.068067     0.114100
%    51.000000   160.915120     0.067383     0.113900
%    52.000000   160.236331     0.066933     0.113500
%    53.000000   160.850737     0.066283     0.113600
%    54.000000   162.394355     0.065533     0.113600
%    55.000000   159.727089     0.064967     0.112900
%    56.000000   160.912580     0.064433     0.112800
%    57.000000   160.015957     0.063833     0.113100
%    58.000000   160.168458     0.063317     0.112700
%    59.000000   160.236180     0.063067     0.112100
%    60.000000   163.929763     0.062650     0.112500
%    61.000000   159.992052     0.062100     0.112700
%    62.000000   160.263242     0.061483     0.112700
%    63.000000   162.919487     0.060950     0.112800
%    64.000000   160.106474     0.060500     0.112500
%    65.000000   157.687046     0.060083     0.112600
%    66.000000   158.003859     0.059667     0.113300
%    67.000000   157.441095     0.059167     0.112600
%    68.000000   157.503690     0.058867     0.113000
%    69.000000   157.081018     0.058417     0.112500
%    70.000000   157.600677     0.058100     0.111900


% used learning variable rate

%cnt =  60000
%nel =  60000
%cnt =  10000
%nel =  10000
%ans =  0.36702
%     1.00000   150.54886     0.15833     0.17230
%     2.00000   149.12430     0.14198     0.15560
%     3.00000   149.22127     0.13433     0.15110
%     4.00000   149.68937     0.12465     0.14380
%     5.00000   149.53596     0.11675     0.13580
%     6.00000   148.88285     0.11068     0.13250
%     7.00000   149.22523     0.10635     0.12790
%     8.00000   149.15580     0.10317     0.12660
%     9.00000   149.28238     0.10033     0.12550
%    10.000000   149.320350     0.098167     0.123300
%    11.000000   149.074390     0.096550     0.123500
%    12.000000   149.400078     0.093917     0.123300
%    13.000000   148.880769     0.092150     0.120600
%    14.000000   148.786436     0.090183     0.120500
%    15.000000   148.772795     0.087467     0.119800
%    16.000000   149.444860     0.085550     0.118900
%    17.000000   149.211438     0.084050     0.119000
%    18.000000   149.126236     0.082600     0.117200
%    19.000000   149.182725     0.081683     0.117900
%    20.000000   148.788491     0.080317     0.116300
%    21.000000   149.577552     0.079050     0.116000
%    22.000000   149.511622     0.078683     0.117200
%    23.000000   149.344781     0.077450     0.118300
%    24.000000   149.293552     0.075933     0.118400
%    25.000000   149.183748     0.076000     0.118400
%learningRate dropped to
%learningRate =  0.090000
%    26.000000   148.936330     0.073933     0.118800
%    27.000000   149.403249     0.074317     0.119100
%learningRate dropped to
%learningRate =  0.081000
%    28.000000   149.814225     0.071733     0.116900
%    29.000000   149.687109     0.071100     0.118100
%    30.000000   149.639917     0.070083     0.117600
%    31.000000   149.162200     0.069383     0.117900
%    32.000000   149.001416     0.068383     0.118100
%    33.000000   148.966743     0.067933     0.117000
%    34.000000   149.698485     0.067267     0.116400
%    35.000000   149.347836     0.066567     0.116100
%    36.000000   148.947970     0.066667     0.116500
%learningRate dropped to
%learningRate =  0.072900
%    37.000000   149.024971     0.065133     0.114500
%    38.000000   149.583748     0.064967     0.114900
%    39.000000   149.402837     0.064650     0.114800
%    40.000000   149.424585     0.064167     0.114500
%    41.000000   149.314505     0.063867     0.114600
%    42.000000   148.974765     0.063467     0.114100
%    43.000000   149.885142     0.063183     0.114500
%    44.000000   150.288790     0.062800     0.114500
%    45.000000   149.236204     0.062250     0.114800
%    46.000000   148.839074     0.061983     0.113600
%    47.000000   148.471695     0.061683     0.113300
%    48.000000   148.557238     0.060983     0.112900
%    49.000000   149.744187     0.060467     0.112800
%    50.000000   149.059544     0.060150     0.113300
%    51.000000   148.837422     0.060250     0.113700
%learningRate dropped to
%learningRate =  0.065610
%    52.000000   149.086112     0.058350     0.113600
%    53.000000   149.347457     0.057750     0.113900
%    54.000000   149.008932     0.057500     0.114400
%    55.000000   148.738392     0.057183     0.115100
%    56.000000   149.650351     0.057067     0.114400
%    57.000000   149.630767     0.057017     0.115200
%    58.000000   149.826513     0.057350     0.115200
%learningRate dropped to
%learningRate =  0.059049
%    59.000000   149.884680     0.056950     0.115100
%    60.000000   149.336583     0.056850     0.115600
%    61.000000   150.112426     0.056583     0.115500
%    62.000000   150.347168     0.055967     0.115400
%    63.000000   149.864767     0.055450     0.115800
%    64.000000   149.229182     0.054667     0.114800
%    65.000000   149.594697     0.054350     0.114700
%    66.000000   149.644117     0.053817     0.115200
%    67.000000   150.323210     0.053233     0.115100
%    68.000000   150.373876     0.052667     0.115000
%    69.000000   149.417430     0.051850     0.115100
%    70.000000   149.705261     0.050950     0.115200
%    71.000000   150.201955     0.050900     0.114800
%    72.000000   149.065825     0.050700     0.114800
%    73.000000   149.429766     0.050100     0.114000
%    74.000000   150.458216     0.049817     0.113800
%    75.000000   149.624584     0.049417     0.113900
%    76.000000   149.217093     0.049617     0.113500
%learningRate dropped to
%learningRate =  0.053144
%    77.000000   149.084579     0.048933     0.113800
%    78.000000   149.539783     0.048550     0.113700
%    79.000000   148.897859     0.048167     0.113700
%    80.000000   148.912383     0.047917     0.113900
%    81.000000   149.390711     0.047617     0.113700
%    82.000000   150.010469     0.047617     0.113700
%learningRate dropped to
%learningRate =  0.047830
%    83.000000   149.956141     0.047283     0.113900
%    84.000000   149.522936     0.047233     0.113900
%    85.000000   149.518906     0.046817     0.113800
%    86.000000   150.553303     0.046783     0.114000
%    87.000000   149.194260     0.046483     0.113300
%    88.000000   148.653373     0.046300     0.113100
%    89.000000   149.184615     0.046683     0.113000
%learningRate dropped to
%learningRate =  0.043047
%    90.000000   149.432101     0.045783     0.112400
%    91.000000   148.972035     0.045483     0.112500
%    92.000000   149.550070     0.045150     0.112100
%    93.000000   149.834664     0.044767     0.111800
%    94.000000   149.584509     0.044517     0.111200
%    95.000000   149.972523     0.044283     0.111700
%    96.000000   148.924828     0.043950     0.111400
%    97.000000   150.095624     0.043683     0.111600
%    98.000000   148.924782     0.043517     0.111100
%    99.000000   148.856896     0.043400     0.111000
%   100.000000   149.056027     0.043283     0.111300
%   101.000000   149.564675     0.042783     0.112000
%   102.000000   150.027294     0.042700     0.111200
%   103.000000   149.495988     0.042450     0.111800
%   104.000000   150.104699     0.042267     0.112600
%   105.000000   149.466095     0.041883     0.112500
%   106.000000   149.376469     0.041650     0.112500
%   107.000000   149.201020     0.041617     0.112100
%   108.000000   149.796249     0.041433     0.112300
%   109.000000   148.554737     0.041133     0.111700
%   110.000000   149.124524     0.041017     0.111400
%   111.000000   150.048909     0.041100     0.111500
%learningRate dropped to
%learningRate =  0.038742
%   112.000000   150.364891     0.040367     0.111000
%   113.000000   149.917856     0.040433     0.111400
%learningRate dropped to
%learningRate =  0.034868
%   114.000000   149.301890     0.039650     0.111400
%   115.000000   149.760298     0.039300     0.111500
%   116.000000   149.755191     0.039217     0.111100
%   117.000000   150.186458     0.039000     0.110900
%   118.000000   150.268054     0.038850     0.110600
%   119.000000   149.815160     0.038617     0.110400
%   120.000000   149.862754     0.038450     0.110900
%   121.000000   149.873917     0.038367     0.111000
%   122.000000   149.584374     0.038300     0.112000
%   123.000000   150.467254     0.038233     0.111900
%   124.000000   150.278646     0.038183     0.111400
%   125.000000   149.467148     0.037917     0.110900
%   126.000000   150.094772     0.037783     0.111000
%   127.000000   149.820626     0.037400     0.110600
%   128.000000   149.785892     0.037283     0.110700
%   129.000000   149.528507     0.037133     0.110200
%   130.000000   150.117651     0.037000     0.110200
%   131.000000   149.257221     0.036667     0.110300
%   132.000000   150.432725     0.036517     0.110000
%   133.000000   150.507123     0.036233     0.110300
%   134.000000   149.937091     0.036200     0.110400
%   135.000000   150.056223     0.035950     0.111300
%   136.000000   150.209901     0.035867     0.111200
%   137.000000   150.586719     0.035700     0.110100
%   138.000000   150.099622     0.035550     0.109700
%   139.000000   149.960168     0.035400     0.109600
%   140.000000   149.905994     0.035167     0.109600
%   141.000000   149.595934     0.034883     0.109800
%   142.000000   149.747573     0.034733     0.110000
%   143.000000   149.850357     0.034617     0.110100
%   144.000000   149.267212     0.034517     0.110400
%   145.000000   149.766940     0.034233     0.110700
%   146.000000   150.488286     0.033933     0.110600
%   147.000000   149.564180     0.033750     0.110300
%   148.000000   149.733036     0.033600     0.111000
%   149.000000   149.013866     0.033583     0.110300
%   150.000000   149.536610     0.033483     0.110500
%   151.000000   149.597327     0.033400     0.110900
%   152.000000   149.769710     0.033333     0.110900
%   153.000000   149.789345     0.033267     0.110900
%   154.000000   149.477839     0.033100     0.110800
%   155.000000   155.530917     0.032867     0.110700
%   156.000000   149.176733     0.032967     0.110700
%learningRate dropped to
%learningRate =  0.031381
%   157.000000   149.786844     0.032467     0.109600
%   158.000000   149.197864     0.032417     0.110300
%   159.000000   148.994262     0.032417     0.110100
%learningRate dropped to
%learningRate =  0.028243
%   160.000000   148.829649     0.032417     0.110100
%learningRate dropped to
%learningRate =  0.025419
%   161.000000   149.942099     0.032100     0.110100
%   162.000000   149.920141     0.031867     0.110300
%   163.000000   151.626603     0.031717     0.110200
%   164.000000   149.448638     0.031517     0.110200
%   165.000000   148.842765     0.031417     0.110000
%   166.000000   148.660792     0.031283     0.109700
%   167.000000   149.103758     0.031133     0.109600
%   168.000000   150.067829     0.031150     0.109500
%learningRate dropped to
%learningRate =  0.022877
%   169.000000   149.116603     0.030550     0.109700
%   170.000000   149.140878     0.030350     0.109600
%   171.000000   149.388268     0.030267     0.109300
%   172.000000   149.103347     0.030100     0.109500
%   173.000000   149.144985     0.029983     0.109200
%   174.000000   149.877489     0.029867     0.109500
%   175.000000   149.481366     0.029817     0.110000
%   176.000000   149.247320     0.029667     0.109800
%   177.000000   149.361420     0.029650     0.109800
%   178.000000   150.125327     0.029500     0.109700
%   179.000000   148.782305     0.029383     0.109500
%   180.000000   149.032513     0.029283     0.109400
%   181.000000   149.282889     0.029100     0.108700
%   182.000000   149.047718     0.028900     0.108500
%   183.000000   149.658716     0.028817     0.108800
%   184.000000   149.838633     0.028750     0.108500
%   185.000000   149.460322     0.028650     0.108600
%   186.000000   148.616827     0.028600     0.108700
%   187.000000   148.959298     0.028483     0.108900
%   188.000000   149.044140     0.028500     0.109000
%learningRate dropped to
%learningRate =  0.020589
%   189.000000   149.153848     0.028350     0.109100
%   190.000000   149.038856     0.028200     0.108900
%   191.000000   149.287645     0.028050     0.109000
%   192.000000   148.869090     0.028017     0.108800
%   193.000000   149.091930     0.027867     0.108600
%   194.000000   149.477834     0.027733     0.108800
%   195.000000   149.735661     0.027700     0.108700
%   196.000000   149.105637     0.027567     0.108800
%   197.000000   149.367446     0.027500     0.108700
%   198.000000   149.556434     0.027333     0.108700
%   199.000000   148.888382     0.027200     0.108800
%   200.000000   148.915375     0.027133     0.108700


% TODO: normalization trick hasnt been evaluated yet


% idea: maybe use regularization?