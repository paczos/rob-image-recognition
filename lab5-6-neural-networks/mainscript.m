% this small NN for tiny data should be 100% correct

% now you can (probably) play with ann_training

% use PCA to reduce dimensionality  
% other method: use PCA and use all dimensions -> decorrelate data

% results must be better than the reference, 
% the logic of classification function could be different, now a simple max is used
% 
% passing: basic backpropagation for a network, calc derivative, 
% 
% fastest path may not be the best in this case, more than one element
% this ds has never been used before, free to suggest new methods
% 
% when to stop trainig? maybe we should wait longer?
%
% seeded random -> in project files there should be serialized state of generator -> repeatability
% 
% 
% 
%


% best so far: 0.114200



[tvec tlab tstv tstl] = readSets();
limSize = rows(tvec);
%limSize = 1000;
limIdx = randperm(limSize);

tlab += 1;
tstl += 1;

tlab = tlab(limIdx, :);
tvec = tvec(limIdx, :);

noHiddenNeurons = 400;
noEpochs = 200;
learningRate = 0.1;
learningDropFrac = 0.1;
minDropRate = 0.0005;
regularization = 0.001;

rand()
rndstate = rand("state");
% save rndstate.txt rndstate
load rndstate.txt
rand("state", rndstate);

[hlnn olnn] = crann(columns(tvec), noHiddenNeurons, 10);
trainError = zeros(1, noEpochs);
testError = zeros(1, noEpochs);
trReport = [];

prevError = 1;
for epoch=1:noEpochs
	tic();
	[hlnn olnn terr] = backprop(tvec, tlab, hlnn, olnn, learningRate, regularization);
	clsRes = anncls(tvec, hlnn, olnn);
	cfmx = confMx(tlab, clsRes);   % it is worth looking into the cfmx in order to know which clothes are incorrectly classified
	errcf = compErrors(cfmx);
	trainError(epoch) = errcf(2);

	clsRes = anncls(tstv, hlnn, olnn);
	cfmx = confMx(tstl, clsRes);
	errcf2 = compErrors(cfmx);
	testError(epoch) = errcf2(2);
	epochTime = toc();
	disp([epoch epochTime trainError(epoch) testError(epoch)])
	trReport = [trReport; epoch epochTime trainError(epoch) testError(epoch)];

	if (trainError(epoch) >= prevError)
        if(minDropRate < learningRate)
            learningRate *= (1-learningDropFrac);
            disp("learningRate dropped to")
            learningRate
        end
	end

    prevError = trainError(epoch);
	fflush(stdout);
end
learningDropTimes


% to beat:
% train: 90.38% 9.62% 0.00%
% test: 87.42% 12.58% 0.00%

% vanilla implementation + params tweaking:
% 0.058100     0.111900

%noHiddenNeurons = 200;
%noEpochs = 50;
%learningRate = 0.01;
%ans =  0.17400
%    1.00000   79.63110    0.23483    0.00000
%    2.00000   79.52931    0.18032    0.00000
%   3.00000   79.36038    0.16568    0.00000



%noHiddenNeurons = 400;
%noEpochs = 60;
%learningRate = 0.015;

%cnt =  60000
%nel =  60000
%cnt =  10000
%nel =  10000
%limSize =  60000
%ans =  0.99220
%     1.00000   158.12970     0.19852     0.20910
%     2.00000   158.73796     0.16570     0.17950
%     3.00000   158.90055     0.15362     0.16870
%     4.00000   158.75812     0.14622     0.16340
%     5.00000   159.11237     0.14167     0.15790
%     6.00000   158.16983     0.13813     0.15320
%     7.00000   158.66217     0.13397     0.15030
%     8.00000   158.99951     0.13125     0.14790
%     9.00000   158.16743     0.12847     0.14690
%    10.00000   158.67155     0.12585     0.14500
%    11.00000   158.09115     0.12380     0.14230
%    12.00000   158.43752     0.12172     0.14170
%    13.00000   157.91327     0.12007     0.14010
%    14.00000   157.85690     0.11817     0.13840
%    15.00000   158.38921     0.11640     0.13760
%    16.00000   158.06481     0.11453     0.13570
%    17.00000   158.19411     0.11290     0.13410
%    18.00000   158.23308     0.11135     0.13340
%    19.00000   156.77038     0.10965     0.13270
%    20.00000   159.27643     0.10803     0.13180
%    21.00000   156.86002     0.10663     0.13100
%    22.00000   156.72536     0.10567     0.12990
%    23.00000   157.16665     0.10453     0.12940
%    24.00000   157.15799     0.10350     0.12820
%    25.00000   156.60208     0.10230     0.12750
%    26.00000   156.70280     0.10137     0.12680
%    27.00000   156.93318     0.10058     0.12540
%    28.000000   157.205459     0.099400     0.124800
%    29.000000   156.754662     0.098233     0.124600
%    30.000000   156.397384     0.097417     0.123100
%    31.000000   156.759514     0.096267     0.123100
%    32.000000   156.293580     0.095283     0.122700
%    33.000000   156.751140     0.094433     0.122500
%    34.000000   157.106400     0.093500     0.121900
%    35.000000   157.000432     0.092483     0.121400
%    36.000000   156.894188     0.091600     0.121200
%    37.000000   156.674338     0.090800     0.121000
%    38.000000   156.887200     0.089917     0.120500
%    39.000000   157.058863     0.089183     0.119500
%    40.000000   156.684943     0.088167     0.119400
%    41.000000   157.944792     0.087383     0.119200
%    42.000000   157.033987     0.086450     0.118700
%    43.000000   157.632600     0.085517     0.118700
%    44.000000   157.369003     0.084867     0.118300
%    45.000000   156.518994     0.083850     0.118900
%    46.000000   156.914155     0.082883     0.119000
%    47.000000   157.467678     0.081900     0.118300
%    48.000000   156.623075     0.081133     0.117900
%    49.000000   157.144239     0.080233     0.117200
%    50.000000   156.344235     0.079633     0.116700
%    51.000000   157.155496     0.078917     0.116300
%    52.000000   156.682843     0.078283     0.116500
%    53.000000   156.637566     0.077717     0.116300
%    54.000000   156.764728     0.077033     0.115700
%    55.000000   156.958661     0.076400     0.115200
%    56.000000   157.207272     0.075783     0.114500
%    57.000000   156.944841     0.075167     0.114400
%    58.000000   156.211678     0.074350     0.114400
%    59.000000   156.574942     0.073817     0.114100
%    60.000000   156.913653     0.073183     0.114200

% used same params but smaller train set:
%nel =  60000
%cnt =  10000
%nel =  10000
%ans =  0.49906
%   1.00000   3.12805   0.89600   0.90000
%   2.00000   3.05713   0.89600   0.90000
%   3.00000   3.07440   0.88300   0.89150
%   4.00000   3.09342   0.69600   0.72300
%   5.00000   3.07737   0.62800   0.64830
%   6.00000   3.08353   0.52700   0.54490
%   7.00000   3.11141   0.43300   0.45960
%   8.00000   3.06046   0.39800   0.42260
%   9.00000   3.11653   0.38900   0.40850
%   10.00000    3.10169    0.37200    0.39900
%   11.00000    3.07615    0.35900    0.38940
%   12.00000    3.19004    0.33400    0.37820
%   13.00000    3.14540    0.31100    0.35950
%   14.00000    3.11769    0.30400    0.33880
%   15.00000    3.04258    0.28200    0.32170
%   16.00000    3.05758    0.27100    0.30860
%   17.00000    3.00996    0.26300    0.30090
%   18.00000    3.06576    0.25700    0.29620
%   19.00000    3.02580    0.24800    0.28940
%   20.00000    3.02396    0.24600    0.28440
%   21.00000    3.04087    0.24500    0.27990
%   22.00000    3.09016    0.24100    0.27670
%   23.00000    3.02172    0.23600    0.27310
%   24.00000    3.05828    0.23100    0.27240
%   25.00000    3.01759    0.22800    0.26990
%   26.00000    3.06436    0.22400    0.26780
%   27.00000    3.05142    0.21900    0.26540
%   28.00000    3.08287    0.21500    0.26390
%   29.00000    3.09490    0.21400    0.26190
%   30.00000    3.02824    0.20900    0.25940
%   31.00000    3.01382    0.20700    0.25780
%   32.00000    3.06430    0.20200    0.25570
%   33.00000    3.04883    0.19800    0.25280
%   34.00000    3.10517    0.19500    0.25040
%   35.00000    3.05765    0.19200    0.24760
%   36.00000    3.15294    0.18700    0.24580
%   37.00000    3.16059    0.18300    0.24340
%   38.00000    3.22274    0.18100    0.24170
%   39.00000    3.08262    0.17800    0.23900
%   40.00000    3.09476    0.17600    0.23690
%   41.00000    3.05108    0.17100    0.23540
%   42.00000    3.11462    0.16700    0.23460
%   43.00000    3.05425    0.16400    0.23350
%   44.00000    3.04017    0.16200    0.23360
%   45.00000    3.10172    0.15800    0.23230
%   46.00000    3.09367    0.15200    0.23100
%   47.00000    3.04131    0.15000    0.23050
%   48.00000    3.04957    0.14700    0.23010
%   49.00000    3.01380    0.14200    0.22790
%   50.00000    3.01575    0.13900    0.22670
%   51.00000    3.04830    0.13700    0.22570
%   52.00000    3.02527    0.13600    0.22450
%   53.00000    3.04452    0.13600    0.22300
%   54.00000    3.03607    0.13200    0.22230
%   55.00000    3.11844    0.13100    0.22150
%   56.00000    3.15210    0.12600    0.22140
%   57.00000    3.05406    0.12400    0.22090
%   58.00000    3.09013    0.12300    0.22080
%   59.00000    3.03454    0.12300    0.22040
%   60.00000    3.00699    0.12200    0.21970


%noHiddenNeurons = 400;
%noEpochs = 70;
%learningRate = 0.025;
%
%     1.00000   157.62863     0.18157     0.19360
%     2.00000   158.15464     0.15988     0.17290
%     3.00000   158.34585     0.14875     0.16200
%     4.00000   158.50825     0.14017     0.15430
%     5.00000   157.93592     0.13372     0.15060
%     6.00000   157.73897     0.12867     0.14450
%     7.00000   158.41189     0.12420     0.14180
%     8.00000   158.54762     0.12120     0.13850
%     9.00000   158.59928     0.11798     0.13540
%    10.00000   158.15023     0.11540     0.13340
%    11.00000   157.84903     0.11307     0.13190
%    12.00000   158.10328     0.11097     0.13150
%    13.00000   157.89401     0.10855     0.13010
%    14.00000   158.34232     0.10633     0.12860
%    15.00000   158.16199     0.10430     0.12740
%    16.00000   157.69957     0.10222     0.12630
%    17.00000   158.17655     0.10067     0.12420
%    18.000000   157.957865     0.099217     0.123700
%    19.000000   159.122827     0.097667     0.122500
%    20.000000   158.687877     0.096283     0.122700
%    21.000000   158.625791     0.094783     0.122400
%    22.000000   159.110525     0.093050     0.121400
%    23.000000   158.302390     0.091967     0.120500
%    24.000000   159.421185     0.090450     0.119900
%    25.000000   157.585951     0.089117     0.119800
%    26.000000   158.468990     0.087900     0.119700
%    27.000000   157.785390     0.086967     0.118800
%    28.000000   158.821688     0.085783     0.117700
%    29.000000   160.719119     0.084817     0.116800
%    30.000000   160.664733     0.083900     0.116800
%    31.000000   159.841529     0.082817     0.116900
%    32.000000   160.815021     0.081783     0.116000
%    33.000000   161.163265     0.080967     0.116000
%    34.000000   160.955891     0.080000     0.116100
%    35.000000   159.626924     0.079183     0.115400
%    36.000000   159.246136     0.078483     0.115100
%    37.000000   160.250138     0.077767     0.115600
%    38.000000   161.188235     0.076967     0.116400
%    39.000000   159.220070     0.076600     0.116000
%    40.000000   160.098277     0.075817     0.116000
%    41.000000   162.508317     0.074983     0.115400
%    42.000000   160.103202     0.074233     0.115300
%    43.000000   159.342916     0.073300     0.115400
%    44.000000   159.737320     0.072683     0.115500
%    45.000000   161.014097     0.071817     0.115400
%    46.000000   159.447582     0.071267     0.114900
%    47.000000   160.582516     0.070517     0.114200
%    48.000000   159.197235     0.069400     0.113800
%    49.000000   159.195799     0.068767     0.113900
%    50.000000   161.349482     0.068067     0.114100
%    51.000000   160.915120     0.067383     0.113900
%    52.000000   160.236331     0.066933     0.113500
%    53.000000   160.850737     0.066283     0.113600
%    54.000000   162.394355     0.065533     0.113600
%    55.000000   159.727089     0.064967     0.112900
%    56.000000   160.912580     0.064433     0.112800
%    57.000000   160.015957     0.063833     0.113100
%    58.000000   160.168458     0.063317     0.112700
%    59.000000   160.236180     0.063067     0.112100
%    60.000000   163.929763     0.062650     0.112500
%    61.000000   159.992052     0.062100     0.112700
%    62.000000   160.263242     0.061483     0.112700
%    63.000000   162.919487     0.060950     0.112800
%    64.000000   160.106474     0.060500     0.112500
%    65.000000   157.687046     0.060083     0.112600
%    66.000000   158.003859     0.059667     0.113300
%    67.000000   157.441095     0.059167     0.112600
%    68.000000   157.503690     0.058867     0.113000
%    69.000000   157.081018     0.058417     0.112500
%    70.000000   157.600677     0.058100     0.111900


% used learning variable rate




% idea: maybe use regularization?